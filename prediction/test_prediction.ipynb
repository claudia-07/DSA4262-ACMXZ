{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workflow\n",
    "\n",
    "1. Install requirements\n",
    "2. Load in data to be predicted\n",
    "3. Load the model to be used for prediction\n",
    "4. Parse the data into dataframes\n",
    "5. Perform pre-processing on dataframes\n",
    "6. Predict scores for all datasets\n",
    "7. Save predictions into CSV files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Install requirements required to run the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting category_encoders==2.5.0\n",
      "  Using cached category_encoders-2.5.0-py2.py3-none-any.whl (69 kB)\n",
      "Collecting hyperopt==0.2.7\n",
      "  Using cached hyperopt-0.2.7-py2.py3-none-any.whl (1.6 MB)\n",
      "Requirement already satisfied: imbalanced-learn==0.9.1 in /Users/xinrantao/opt/anaconda3/envs/DSA3101/lib/python3.8/site-packages (from -r ../requirements.txt (line 3)) (0.9.1)\n",
      "Collecting ipython==8.6.0\n",
      "  Using cached ipython-8.6.0-py3-none-any.whl (761 kB)\n",
      "Collecting pandas==1.2.5\n",
      "  Using cached pandas-1.2.5-cp38-cp38-macosx_10_9_x86_64.whl (10.5 MB)\n",
      "Collecting pip==22.2.2\n",
      "  Using cached pip-22.2.2-py3-none-any.whl (2.0 MB)\n",
      "Collecting scikit-learn==1.1.2\n",
      "  Using cached scikit_learn-1.1.2-cp38-cp38-macosx_10_9_x86_64.whl (8.6 MB)\n",
      "Collecting shap==0.39.0\n",
      "  Using cached shap-0.39.0-cp38-cp38-macosx_10_9_x86_64.whl\n",
      "Collecting fastparquet\n",
      "  Using cached fastparquet-0.8.3-cp38-cp38-macosx_10_9_x86_64.whl (602 kB)\n",
      "Collecting pyarrow==10.0.0\n",
      "  Using cached pyarrow-10.0.0-cp38-cp38-macosx_10_14_x86_64.whl (24.6 MB)\n",
      "Collecting joblib==1.2.0\n",
      "  Using cached joblib-1.2.0-py3-none-any.whl (297 kB)\n",
      "Collecting lightgbm==3.2.1\n",
      "  Using cached lightgbm-3.2.1-py3-none-macosx_10_14_x86_64.macosx_10_15_x86_64.macosx_11_0_x86_64.whl (1.2 MB)\n",
      "Requirement already satisfied: statsmodels>=0.9.0 in /Users/xinrantao/opt/anaconda3/envs/DSA3101/lib/python3.8/site-packages (from category_encoders==2.5.0->-r ../requirements.txt (line 1)) (0.12.2)\n",
      "Requirement already satisfied: numpy>=1.14.0 in /Users/xinrantao/opt/anaconda3/envs/DSA3101/lib/python3.8/site-packages (from category_encoders==2.5.0->-r ../requirements.txt (line 1)) (1.19.5)\n",
      "Requirement already satisfied: patsy>=0.5.1 in /Users/xinrantao/opt/anaconda3/envs/DSA3101/lib/python3.8/site-packages (from category_encoders==2.5.0->-r ../requirements.txt (line 1)) (0.5.1)\n",
      "Requirement already satisfied: scipy>=1.0.0 in /Users/xinrantao/opt/anaconda3/envs/DSA3101/lib/python3.8/site-packages (from category_encoders==2.5.0->-r ../requirements.txt (line 1)) (1.7.1)\n",
      "Collecting future\n",
      "  Using cached future-0.18.2-py3-none-any.whl\n",
      "Collecting networkx>=2.2\n",
      "  Downloading networkx-2.8.8-py3-none-any.whl (2.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.0 MB 1.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting cloudpickle\n",
      "  Downloading cloudpickle-2.2.0-py3-none-any.whl (25 kB)\n",
      "Requirement already satisfied: six in /Users/xinrantao/opt/anaconda3/envs/DSA3101/lib/python3.8/site-packages (from hyperopt==0.2.7->-r ../requirements.txt (line 2)) (1.16.0)\n",
      "Collecting tqdm\n",
      "  Downloading tqdm-4.64.1-py2.py3-none-any.whl (78 kB)\n",
      "\u001b[K     |████████████████████████████████| 78 kB 3.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting py4j\n",
      "  Using cached py4j-0.10.9.7-py2.py3-none-any.whl (200 kB)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/xinrantao/opt/anaconda3/envs/DSA3101/lib/python3.8/site-packages (from imbalanced-learn==0.9.1->-r ../requirements.txt (line 3)) (2.2.0)\n",
      "Requirement already satisfied: appnope in /Users/xinrantao/opt/anaconda3/envs/DSA3101/lib/python3.8/site-packages (from ipython==8.6.0->-r ../requirements.txt (line 4)) (0.1.2)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /Users/xinrantao/opt/anaconda3/envs/DSA3101/lib/python3.8/site-packages (from ipython==8.6.0->-r ../requirements.txt (line 4)) (2.9.0)\n",
      "Requirement already satisfied: jedi>=0.16 in /Users/xinrantao/opt/anaconda3/envs/DSA3101/lib/python3.8/site-packages (from ipython==8.6.0->-r ../requirements.txt (line 4)) (0.18.0)\n",
      "Requirement already satisfied: decorator in /Users/xinrantao/opt/anaconda3/envs/DSA3101/lib/python3.8/site-packages (from ipython==8.6.0->-r ../requirements.txt (line 4)) (5.0.9)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>3.0.1 in /Users/xinrantao/opt/anaconda3/envs/DSA3101/lib/python3.8/site-packages (from ipython==8.6.0->-r ../requirements.txt (line 4)) (3.0.19)\n",
      "Requirement already satisfied: backcall in /Users/xinrantao/opt/anaconda3/envs/DSA3101/lib/python3.8/site-packages (from ipython==8.6.0->-r ../requirements.txt (line 4)) (0.2.0)\n",
      "Collecting stack-data\n",
      "  Using cached stack_data-0.6.0-py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: pickleshare in /Users/xinrantao/opt/anaconda3/envs/DSA3101/lib/python3.8/site-packages (from ipython==8.6.0->-r ../requirements.txt (line 4)) (0.7.5)\n",
      "Requirement already satisfied: pexpect>4.3 in /Users/xinrantao/opt/anaconda3/envs/DSA3101/lib/python3.8/site-packages (from ipython==8.6.0->-r ../requirements.txt (line 4)) (4.8.0)\n",
      "Requirement already satisfied: traitlets>=5 in /Users/xinrantao/opt/anaconda3/envs/DSA3101/lib/python3.8/site-packages (from ipython==8.6.0->-r ../requirements.txt (line 4)) (5.0.5)\n",
      "Requirement already satisfied: matplotlib-inline in /Users/xinrantao/opt/anaconda3/envs/DSA3101/lib/python3.8/site-packages (from ipython==8.6.0->-r ../requirements.txt (line 4)) (0.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /Users/xinrantao/opt/anaconda3/envs/DSA3101/lib/python3.8/site-packages (from pandas==1.2.5->-r ../requirements.txt (line 5)) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /Users/xinrantao/opt/anaconda3/envs/DSA3101/lib/python3.8/site-packages (from pandas==1.2.5->-r ../requirements.txt (line 5)) (2021.1)\n",
      "Collecting slicer==0.0.7\n",
      "  Using cached slicer-0.0.7-py3-none-any.whl (14 kB)\n",
      "Collecting numba\n",
      "  Downloading numba-0.56.4-cp38-cp38-macosx_10_14_x86_64.whl (2.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.4 MB 2.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: wheel in /Users/xinrantao/opt/anaconda3/envs/DSA3101/lib/python3.8/site-packages (from lightgbm==3.2.1->-r ../requirements.txt (line 12)) (0.36.2)\n",
      "Collecting cramjam>=2.3.0\n",
      "  Using cached cramjam-2.6.1-cp38-cp38-macosx_10_9_x86_64.macosx_11_0_arm64.macosx_10_9_universal2.whl (3.2 MB)\n",
      "Collecting fsspec\n",
      "  Using cached fsspec-2022.10.0-py3-none-any.whl (138 kB)\n",
      "Requirement already satisfied: packaging in /Users/xinrantao/opt/anaconda3/envs/DSA3101/lib/python3.8/site-packages (from fastparquet->-r ../requirements.txt (line 9)) (21.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /Users/xinrantao/opt/anaconda3/envs/DSA3101/lib/python3.8/site-packages (from jedi>=0.16->ipython==8.6.0->-r ../requirements.txt (line 4)) (0.8.2)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /Users/xinrantao/opt/anaconda3/envs/DSA3101/lib/python3.8/site-packages (from pexpect>4.3->ipython==8.6.0->-r ../requirements.txt (line 4)) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /Users/xinrantao/opt/anaconda3/envs/DSA3101/lib/python3.8/site-packages (from prompt-toolkit<3.1.0,>3.0.1->ipython==8.6.0->-r ../requirements.txt (line 4)) (0.2.5)\n",
      "Requirement already satisfied: ipython-genutils in /Users/xinrantao/opt/anaconda3/envs/DSA3101/lib/python3.8/site-packages (from traitlets>=5->ipython==8.6.0->-r ../requirements.txt (line 4)) (0.2.0)\n",
      "Requirement already satisfied: setuptools in /Users/xinrantao/opt/anaconda3/envs/DSA3101/lib/python3.8/site-packages (from numba->shap==0.39.0->-r ../requirements.txt (line 8)) (52.0.0.post20210125)\n",
      "Collecting importlib-metadata\n",
      "  Downloading importlib_metadata-5.0.0-py3-none-any.whl (21 kB)\n",
      "Collecting llvmlite<0.40,>=0.39.0dev0\n",
      "  Downloading llvmlite-0.39.1-cp38-cp38-macosx_10_9_x86_64.whl (25.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 25.5 MB 2.3 MB/s eta 0:00:01     |███████████████████████         | 18.2 MB 4.2 MB/s eta 0:00:02\n",
      "\u001b[?25hCollecting zipp>=0.5\n",
      "  Downloading zipp-3.10.0-py3-none-any.whl (6.2 kB)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /Users/xinrantao/opt/anaconda3/envs/DSA3101/lib/python3.8/site-packages (from packaging->fastparquet->-r ../requirements.txt (line 9)) (2.4.7)\n",
      "Collecting asttokens>=2.1.0\n",
      "  Using cached asttokens-2.1.0-py2.py3-none-any.whl (26 kB)\n",
      "Collecting executing>=1.2.0\n",
      "  Using cached executing-1.2.0-py2.py3-none-any.whl (24 kB)\n",
      "Collecting pure-eval\n",
      "  Using cached pure_eval-0.2.2-py3-none-any.whl (11 kB)\n",
      "Installing collected packages: zipp, pure-eval, pandas, llvmlite, joblib, importlib-metadata, executing, asttokens, tqdm, stack-data, slicer, scikit-learn, py4j, numba, networkx, future, fsspec, cramjam, cloudpickle, shap, pyarrow, pip, lightgbm, ipython, hyperopt, fastparquet, category-encoders\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 1.3.1\n",
      "    Uninstalling pandas-1.3.1:\n",
      "      Successfully uninstalled pandas-1.3.1\n",
      "  Attempting uninstall: joblib\n",
      "    Found existing installation: joblib 1.0.1\n",
      "    Uninstalling joblib-1.0.1:\n",
      "      Successfully uninstalled joblib-1.0.1\n",
      "  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 1.1.3\n",
      "    Uninstalling scikit-learn-1.1.3:\n",
      "      Successfully uninstalled scikit-learn-1.1.3\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 21.2.2\n",
      "    Uninstalling pip-21.2.2:\n",
      "      Successfully uninstalled pip-21.2.2\n",
      "  Attempting uninstall: ipython\n",
      "    Found existing installation: ipython 7.26.0\n",
      "    Uninstalling ipython-7.26.0:\n",
      "      Successfully uninstalled ipython-7.26.0\n",
      "  Attempting uninstall: category-encoders\n",
      "    Found existing installation: category-encoders 2.5.1.post0\n",
      "    Uninstalling category-encoders-2.5.1.post0:\n",
      "      Successfully uninstalled category-encoders-2.5.1.post0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "kaggle 1.5.12 requires requests, which is not installed.\n",
      "kaggle 1.5.12 requires urllib3, which is not installed.\n",
      "ipykernel 6.0.3 requires ipython<8.0,>=7.23.1, but you have ipython 8.6.0 which is incompatible.\u001b[0m\n",
      "Successfully installed asttokens-2.1.0 category-encoders-2.5.0 cloudpickle-2.2.0 cramjam-2.6.1 executing-1.2.0 fastparquet-0.8.3 fsspec-2022.10.0 future-0.18.2 hyperopt-0.2.7 importlib-metadata-5.0.0 ipython-8.6.0 joblib-1.2.0 lightgbm-3.2.1 llvmlite-0.39.1 networkx-2.8.8 numba-0.56.4 pandas-1.2.5 pip-22.2.2 pure-eval-0.2.2 py4j-0.10.9.7 pyarrow-10.0.0 scikit-learn-1.1.2 shap-0.39.0 slicer-0.0.7 stack-data-0.6.0 tqdm-4.64.1 zipp-3.10.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Load in data to be predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"../data/small_test_data.json\"\n",
    "data = [json.loads(line) for line in open(data_path, 'r')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Load the model to be used for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfe_features = ['std_-1_25', 'std_-1_50', 'std_-1_75', 'std_-1_mean', 'std_-1_min',\n",
    "       'mean_-1_25', 'mean_-1_50', 'mean_-1_75', 'mean_-1_mean', 'mean_-1_min',\n",
    "       'dwelling_time_0_50', 'dwelling_time_0_mean', 'std_0_25', 'std_0_50',\n",
    "       'std_0_75', 'std_0_mean', 'std_0_min', 'std_0_max', 'mean_0_25',\n",
    "       'mean_0_50', 'mean_0_75', 'mean_0_mean', 'mean_0_min', 'mean_0_max',\n",
    "       'dwelling_time_+1_mean', 'std_+1_25', 'std_+1_50', 'mean_+1_25',\n",
    "       'mean_+1_50', 'mean_+1_75', 'mean_+1_mean', 'mean_+1_min',\n",
    "       'mean_+1_max', 'relative_position', 'position_1_G', 'position_5_T']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pickled random forest model\n",
    "import joblib\n",
    "pickled_model = joblib.load('../deployment/rf.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Parse the Data into Dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## function to get key of a dictionary\n",
    "def get_key(dictionary):\n",
    "    key_object = dictionary.keys()\n",
    "    key = list(key_object)[0]\n",
    "    return key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## function to help concatenate columns to get transcript, position, nucleotides\n",
    "def concat_col(transcript, position, nucleotide, n):\n",
    "    t_df = pd.DataFrame([transcript]*n)\n",
    "    p_df = pd.DataFrame([position]*n)\n",
    "    nu_df = pd.DataFrame([nucleotide]*n)\n",
    "    n_df = pd.DataFrame([n]*n)\n",
    "\n",
    "    ## concat columns together\n",
    "    final_df = pd.concat([t_df, p_df, nu_df, n_df], axis = 1)\n",
    "    final_df.columns = ['transcript', 'position', 'nucleotides', 'reads_count']\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## function to parse line in json file\n",
    "def parse_line(line):\n",
    "    ## get transcript\n",
    "    t = get_key(line)\n",
    "\n",
    "    ## get position\n",
    "    p = get_key(line[t])\n",
    "\n",
    "    ## get nucleotide seq\n",
    "    nu = get_key(line[t][p])\n",
    "\n",
    "    ## get number of reads\n",
    "    reads_count = len(line[t][p][nu])\n",
    "\n",
    "    ## get dataframe of list of reads\n",
    "    reads = pd.DataFrame(line[t][p][nu])\n",
    "\n",
    "    ## concat columns together to get transcript, position, nucleotides and all dwelling time, std, mean\n",
    "    df = pd.concat([concat_col(t, p, nu, reads_count), reads], axis = 1)\n",
    "    df.columns = ['transcript', 'position', 'nucleotides', 'reads_count', 'dwellingtime_-1', 'std_-1', 'mean_-1', 'dwellingtime_0', 'std_0', 'mean_0', 'dwellingtime_+1', 'std_+1', 'mean_+1']\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(data):\n",
    "    ## parse all lines into dataframes\n",
    "    reads = [parse_line(data[0][i]) for i in range(len(data[0]))]\n",
    "    print(len(reads))\n",
    "\n",
    "    ## concatenate dataframes\n",
    "    result_df = pd.concat(reads, axis = 0)\n",
    "    print(f\"Shape of Dataset = {result_df.shape}\")\n",
    "\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500\n",
      "Shape of Dataset = (141681, 13)\n"
     ]
    }
   ],
   "source": [
    "data_df = parse(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Perform Pre-Processing on Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from category_encoders import OneHotEncoder\n",
    "\n",
    "sys.path.append(os.path.abspath(\"../util/model\"))\n",
    "from training import get_percent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_eng(df):\n",
    "    temp = pd.DataFrame(df.groupby(['transcript', 'position', 'nucleotides', 'reads_count'], as_index=False)\n",
    "                           .agg({'dwellingtime_-1': [get_percent(25), get_percent(50), get_percent(75), np.mean, np.min, np.max],\n",
    "                                'std_-1': [get_percent(25), get_percent(50), get_percent(75), np.mean, np.min, np.max],\n",
    "                                'mean_-1': [get_percent(25), get_percent(50), get_percent(75), np.mean, np.min, np.max],\n",
    "                                'dwellingtime_0': [get_percent(25), get_percent(50), get_percent(75), np.mean, np.min, np.max],\n",
    "                                'std_0': [get_percent(25), get_percent(50), get_percent(75), np.mean, np.min, np.max],\n",
    "                                'mean_0': [get_percent(25), get_percent(50), get_percent(75), np.mean, np.min, np.max],\n",
    "                                'dwellingtime_+1': [get_percent(25), get_percent(50), get_percent(75), np.mean, np.min, np.max],\n",
    "                                'std_+1': [get_percent(25), get_percent(50), get_percent(75), np.mean, np.min, np.max],\n",
    "                                'mean_+1': [get_percent(25), get_percent(50), get_percent(75), np.mean, np.min, np.max]}))\n",
    "    temp.columns = ['transcript', 'position', 'nucleotides', 'reads_count',\n",
    "                        'dwelling_time_-1_25', 'dwelling_time_-1_50', 'dwelling_time_-1_75', 'dwelling_time_-1_mean','dwelling_time_-1_min', 'dwelling_time_-1_max',\n",
    "                        'std_-1_25', 'std_-1_50', 'std_-1_75', 'std_-1_mean','std_-1_min', 'std_-1_max',\n",
    "                        'mean_-1_25', 'mean_-1_50', 'mean_-1_75', 'mean_-1_mean','mean_-1_min', 'mean_-1_max',\n",
    "                        'dwelling_time_0_25', 'dwelling_time_0_50', 'dwelling_time_0_75', 'dwelling_time_0_mean','dwelling_time_0_min','dwelling_time_0_max',\n",
    "                        'std_0_25', 'std_0_50', 'std_0_75', 'std_0_mean','std_0_min', 'std_0_max',\n",
    "                        'mean_0_25', 'mean_0_50', 'mean_0_75', 'mean_0_mean','mean_0_min', 'mean_0_max',\n",
    "                        'dwelling_time_+1_25', 'dwelling_time_+1_50', 'dwelling_time_+1_75', 'dwelling_time_+1_mean','dwelling_time_+1_min','dwelling_time_+1_max',\n",
    "                        'std_+1_25', 'std_+1_50', 'std_+1_75', 'std_+1_mean','std_+1_min', 'std_+1_max',\n",
    "                        'mean_+1_25', 'mean_+1_50', 'mean_+1_75', 'mean_+1_mean','mean_+1_min', 'mean_+1_max']\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relative_position(df):\n",
    "    df[\"position\"] = df[\"position\"].astype(int)\n",
    "\n",
    "    ## find relative position of each read in each transcript\n",
    "    df[\"relative_position\"] = df.groupby([\"transcript\"])[\"position\"].transform(lambda x: (x - x.min())/(x.max()-x.min()))\n",
    "\n",
    "    ## note: have NAs because there's transcripts with only one position\n",
    "    ## fill the NAs with 0\n",
    "    df[\"relative_position\"] = df[\"relative_position\"].fillna(0)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## variables needed for encoding\n",
    "pipe = pickle.load(open(\"../data/model_training/raw_data/encoding_pipeline.pkl\", \"rb\"))\n",
    "\n",
    "cols_to_map = ['reads_count', 'dwelling_time_-1_25', 'dwelling_time_-1_50', 'dwelling_time_-1_75', \n",
    "                'dwelling_time_-1_mean', 'dwelling_time_-1_min', 'dwelling_time_-1_max', 'std_-1_25', \n",
    "                'std_-1_50', 'std_-1_75', 'std_-1_mean', 'std_-1_min', 'std_-1_max', 'mean_-1_25', \n",
    "                'mean_-1_50', 'mean_-1_75', 'mean_-1_mean', 'mean_-1_min', 'mean_-1_max', \n",
    "                'dwelling_time_0_25', 'dwelling_time_0_50', 'dwelling_time_0_75', 'dwelling_time_0_mean', \n",
    "                'dwelling_time_0_min', 'dwelling_time_0_max', 'std_0_25', 'std_0_50', 'std_0_75', \n",
    "                'std_0_mean', 'std_0_min', 'std_0_max', 'mean_0_25', 'mean_0_50', 'mean_0_75', 'mean_0_mean', \n",
    "                'mean_0_min', 'mean_0_max', 'dwelling_time_+1_25', 'dwelling_time_+1_50', 'dwelling_time_+1_75', \n",
    "                'dwelling_time_+1_mean', 'dwelling_time_+1_min', 'dwelling_time_+1_max', 'std_+1_25', \n",
    "                'std_+1_50', 'std_+1_75', 'std_+1_mean', 'std_+1_min', 'std_+1_max', 'mean_+1_25', 'mean_+1_50', \n",
    "                'mean_+1_75', 'mean_+1_mean', 'mean_+1_min', 'mean_+1_max', 'relative_position', 'position_0_C', \n",
    "                'position_0_G', 'position_0_T', 'position_0_A', 'position_1_A', 'position_1_G', 'position_1_T', \n",
    "                'position_2_A', 'position_2_G', 'position_3_A', 'position_4_C', 'position_5_C', 'position_5_A', \n",
    "                'position_5_T', 'position_6_T', 'position_6_A', 'position_6_G', 'position_6_C']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoding(df, columns_to_map):\n",
    "    id_val = df[['transcript','position']] ## needed to concat with pred proba for submission\n",
    "\n",
    "    for i in range(7):\n",
    "        df['position_' + str(i)] = df['nucleotides'].apply(lambda x: x[i])\n",
    "    \n",
    "    df_enc = pd.DataFrame({col: vals for vals, col in zip(pipe.transform(df).T, columns_to_map)})\n",
    "\n",
    "    return df_enc, id_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1492, 58)\n",
      "(1492, 59)\n",
      "(1492, 74) (1492, 2)\n"
     ]
    }
   ],
   "source": [
    "percentile_df = feature_eng(data_df)\n",
    "print(percentile_df.shape)\n",
    "\n",
    "relative_position_df = relative_position(percentile_df)\n",
    "print(relative_position_df.shape)\n",
    "\n",
    "encoded_df, id_val_df = encoding(relative_position_df, cols_to_map)\n",
    "print(encoded_df.shape, id_val_df.shape)\n",
    "\n",
    "data_pp = encoded_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Predicting Probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_pickled_model(data_id_col, data_pp):\n",
    "    ## predict using pickled_model\n",
    "    data_pred = pickled_model.predict_proba(data_pp[rfe_features])[:,1]\n",
    "    print(len(data_pred))\n",
    "\n",
    "    ## convert predictions to dataframe\n",
    "    data_pred_df = pd.DataFrame(data_pred, columns = ['score'])\n",
    "\n",
    "    ## \n",
    "    data_pred_df = pd.concat([data_id_col, data_pred_df], axis = 1)\n",
    "    print(f\"Prediction file is of shape: {data_pred_df.shape}\")\n",
    "\n",
    "    return data_pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1492\n",
      "Prediction file is of shape: (1492, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transcript</th>\n",
       "      <th>position</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ENST00000005386</td>\n",
       "      <td>1758</td>\n",
       "      <td>0.009091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ENST00000009041</td>\n",
       "      <td>1086</td>\n",
       "      <td>0.009948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ENST00000009041</td>\n",
       "      <td>955</td>\n",
       "      <td>0.027273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ENST00000012443</td>\n",
       "      <td>701</td>\n",
       "      <td>0.027273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ENST00000013807</td>\n",
       "      <td>822</td>\n",
       "      <td>0.004545</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        transcript  position     score\n",
       "0  ENST00000005386      1758  0.009091\n",
       "1  ENST00000009041      1086  0.009948\n",
       "2  ENST00000009041       955  0.027273\n",
       "3  ENST00000012443       701  0.027273\n",
       "4  ENST00000013807       822  0.004545"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_df = prediction_pickled_model(id_val_df, data_pp)\n",
    "prediction_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Save predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "## save predictions as csv file\n",
    "prediction_fname = \"test_predictions.csv\"\n",
    "prediction_df.to_csv(prediction_fname)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d3ba0bc2f1cd5c3834b09a6def774c389bad0c62f3a6495699baf0c4d1ccd759"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
