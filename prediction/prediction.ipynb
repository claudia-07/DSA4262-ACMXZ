{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workflow\n",
    "\n",
    "1. Identify datasets to be predicted\n",
    "2. Parse the datasets into dataframes\n",
    "3. Perform pre-processing on dataframes\n",
    "4. Train chosen model on full training data\n",
    "5. Predict labels for all datasets\n",
    "6. Save predictions into CSV files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identify datasets to be predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## libraries to read and parse json file\n",
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/claudia/DSA4262-ACMXZ/prediction'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## get current working directory\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse datasets into dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions needed to parse json files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## function to get key of a dictionary\n",
    "def get_key(dictionary):\n",
    "    key_object = dictionary.keys()\n",
    "    key = list(key_object)[0]\n",
    "    return key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## function to help concatenate columns to get transcript, position, nucleotides\n",
    "def concat_col(transcript, position, nucleotide, n):\n",
    "    t_df = pd.DataFrame([transcript]*n)\n",
    "    p_df = pd.DataFrame([position]*n)\n",
    "    nu_df = pd.DataFrame([nucleotide]*n)\n",
    "    n_df = pd.DataFrame([n]*n)\n",
    "\n",
    "    ## concat columns together\n",
    "    final_df = pd.concat([t_df, p_df, nu_df, n_df], axis = 1)\n",
    "    final_df.columns = ['transcript', 'position', 'nucleotides', 'reads_count']\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## function to parse line in json file\n",
    "def parse_line(line):\n",
    "    ## get transcript\n",
    "    t = get_key(line)\n",
    "\n",
    "    ## get position\n",
    "    p = get_key(line[t])\n",
    "\n",
    "    ## get nucleotide seq\n",
    "    nu = get_key(line[t][p])\n",
    "\n",
    "    ## get number of reads\n",
    "    reads_count = len(line[t][p][nu])\n",
    "\n",
    "    ## get dataframe of list of reads\n",
    "    reads = pd.DataFrame(line[t][p][nu])\n",
    "\n",
    "    ## concat columns together to get transcript, position, nucleotides and all dwelling time, std, mean\n",
    "    df = pd.concat([concat_col(t, p, nu, reads_count), reads], axis = 1)\n",
    "    df.columns = ['transcript', 'position', 'nucleotides', 'reads_count', 'dwellingtime_-1', 'std_-1', 'mean_-1', 'dwellingtime_0', 'std_0', 'mean_0', 'dwellingtime_+1', 'std_+1', 'mean_+1']\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## function to breakdown dataframe into smaller sizes and save it\n",
    "def save_file(df, filename, nrows = 2500000):\n",
    "    total_rows = len(df)\n",
    "    start, stop, count = 0, nrows, 1\n",
    "\n",
    "    ## use while loop to break the dataframe into smaller dataframes\n",
    "    while stop < total_rows:\n",
    "        print(start, stop)\n",
    "        temp_df = pd.DataFrame(df.iloc[start:stop, :])\n",
    "        fname = f\"{filename}_{count}.parquet\"\n",
    "        temp_df.to_parquet(fname)\n",
    "        print(f\"Saved a file called {fname}\")\n",
    "        count += 1\n",
    "        start += nrows\n",
    "        stop += nrows\n",
    "    \n",
    "    stop = total_rows\n",
    "    print(start, stop)\n",
    "    temp_df = pd.DataFrame(df.iloc[start:stop, :])\n",
    "    fname = f\"{filename}_{count}.parquet\"\n",
    "    temp_df.to_parquet(fname)\n",
    "    print(f\"Saved a file called {fname}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(file, filename):\n",
    "    ## open file\n",
    "    data = [json.loads(line) for line in open(file, 'r')]\n",
    "\n",
    "    ## parse all lines into dataframes\n",
    "    reads = [parse_line(data[i]) for i in range(len(data))]\n",
    "\n",
    "    ## concatenate dataframes\n",
    "    result_df = pd.concat(reads, axis = 0)\n",
    "    print(f\"Shape of Dataset = {result_df.shape}\")\n",
    "\n",
    "    ## save dataframe into parquet files\n",
    "    save_file(result_df, filename)\n",
    "\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Dataset = (7907952, 13)\n",
      "0 5000000\n",
      "Saved a file called ../data/final_round/dataset1_1.parquet\n",
      "5000000 7907952\n",
      "Saved a file called ../data/final_round/dataset1_2.parquet\n"
     ]
    }
   ],
   "source": [
    "dataset1_path = \"/Users/claudia/Downloads/dataset1.json\"\n",
    "dataset1_filename = \"../data/final_round/dataset1\"\n",
    "dataset1 = parse(dataset1_path, dataset1_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Dataset = (6903936, 13)\n",
      "0 2500000\n",
      "Saved a file called ../data/final_round/dataset2_1.parquet\n",
      "2500000 5000000\n",
      "Saved a file called ../data/final_round/dataset2_2.parquet\n",
      "5000000 6903936\n",
      "Saved a file called ../data/final_round/dataset2_3.parquet\n"
     ]
    }
   ],
   "source": [
    "dataset2_path = \"/Users/claudia/Downloads/dataset2.json\"\n",
    "dataset2_filename = \"../data/final_round/dataset2\"\n",
    "dataset2 = parse(dataset2_path, dataset2_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Dataset = (1171940, 13)\n",
      "0 1171940\n",
      "Saved a file called ../data/final_round/dataset3_1.parquet\n"
     ]
    }
   ],
   "source": [
    "dataset3_path = \"/Users/claudia/Downloads/dataset3.json\"\n",
    "dataset3_filename = \"../data/final_round/dataset3\"\n",
    "dataset3 = parse(dataset3_path, dataset3_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform pre-processing on dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions needed for pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/scipy/__init__.py:138: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.4)\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion} is required for this version of \"\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from category_encoders import OneHotEncoder\n",
    "\n",
    "sys.path.append(os.path.abspath(\"../../util/model\"))\n",
    "from training import get_percent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gene_id and labels removed from groupby and temp.columns\n",
    "def feature_eng(df):\n",
    "    temp = pd.DataFrame(df.groupby(['transcript', 'position', 'nucleotides', 'reads_count'], as_index=False)\n",
    "                           .agg({'dwellingtime_-1': [get_percent(25), get_percent(50), get_percent(75), np.mean, np.min, np.max],\n",
    "                                'std_-1': [get_percent(25), get_percent(50), get_percent(75), np.mean, np.min, np.max],\n",
    "                                'mean_-1': [get_percent(25), get_percent(50), get_percent(75), np.mean, np.min, np.max],\n",
    "                                'dwellingtime_0': [get_percent(25), get_percent(50), get_percent(75), np.mean, np.min, np.max],\n",
    "                                'std_0': [get_percent(25), get_percent(50), get_percent(75), np.mean, np.min, np.max],\n",
    "                                'mean_0': [get_percent(25), get_percent(50), get_percent(75), np.mean, np.min, np.max],\n",
    "                                'dwellingtime_+1': [get_percent(25), get_percent(50), get_percent(75), np.mean, np.min, np.max],\n",
    "                                'std_+1': [get_percent(25), get_percent(50), get_percent(75), np.mean, np.min, np.max],\n",
    "                                'mean_+1': [get_percent(25), get_percent(50), get_percent(75), np.mean, np.min, np.max]}))\n",
    "    temp.columns = ['transcript', 'position', 'nucleotides', 'reads_count',\n",
    "                        'dwelling_time_-1_25', 'dwelling_time_-1_50', 'dwelling_time_-1_75', 'dwelling_time_-1_mean','dwelling_time_-1_min', 'dwelling_time_-1_max',\n",
    "                        'std_-1_25', 'std_-1_50', 'std_-1_75', 'std_-1_mean','std_-1_min', 'std_-1_max',\n",
    "                        'mean_-1_25', 'mean_-1_50', 'mean_-1_75', 'mean_-1_mean','mean_-1_min', 'mean_-1_max',\n",
    "                        'dwelling_time_0_25', 'dwelling_time_0_50', 'dwelling_time_0_75', 'dwelling_time_0_mean','dwelling_time_0_min','dwelling_time_0_max',\n",
    "                        'std_0_25', 'std_0_50', 'std_0_75', 'std_0_mean','std_0_min', 'std_0_max',\n",
    "                        'mean_0_25', 'mean_0_50', 'mean_0_75', 'mean_0_mean','mean_0_min', 'mean_0_max',\n",
    "                        'dwelling_time_+1_25', 'dwelling_time_+1_50', 'dwelling_time_+1_75', 'dwelling_time_+1_mean','dwelling_time_+1_min','dwelling_time_+1_max',\n",
    "                        'std_+1_25', 'std_+1_50', 'std_+1_75', 'std_+1_mean','std_+1_min', 'std_+1_max',\n",
    "                        'mean_+1_25', 'mean_+1_50', 'mean_+1_75', 'mean_+1_mean','mean_+1_min', 'mean_+1_max']\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gene_id removed from groupby\n",
    "def relative_position(df):\n",
    "    df[\"position\"] = df[\"position\"].astype(int)\n",
    "\n",
    "    ## find relative position of each read in each transcript\n",
    "    df[\"relative_position\"] = df.groupby([\"transcript\"])[\"position\"].transform(lambda x: (x - x.min())/(x.max()-x.min()))\n",
    "\n",
    "    ## note: have NAs because there's transcripts with only one position\n",
    "    ## fill the NAs with 0\n",
    "    df[\"relative_position\"] = df[\"relative_position\"].fillna(0)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.8.8 (default, Apr 13 2021, 12:59:45) \\n[Clang 10.0.0 ]'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.2.4'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas==1.5.1\n",
      "  Downloading pandas-1.5.1-cp38-cp38-macosx_10_9_x86_64.whl (11.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 11.9 MB 2.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.8/site-packages (from pandas==1.5.1) (2021.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /opt/anaconda3/lib/python3.8/site-packages (from pandas==1.5.1) (2.8.1)\n",
      "Collecting numpy>=1.20.3\n",
      "  Downloading numpy-1.23.4-cp38-cp38-macosx_10_9_x86_64.whl (18.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 18.1 MB 303 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.8/site-packages (from python-dateutil>=2.8.1->pandas==1.5.1) (1.15.0)\n",
      "Installing collected packages: numpy, pandas\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.20.1\n",
      "    Uninstalling numpy-1.20.1:\n",
      "      Successfully uninstalled numpy-1.20.1\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 1.0.2\n",
      "    Uninstalling pandas-1.0.2:\n",
      "      Successfully uninstalled pandas-1.0.2\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "scipy 1.6.2 requires numpy<1.23.0,>=1.16.5, but you have numpy 1.23.4 which is incompatible.\u001b[0m\n",
      "Successfully installed numpy-1.23.4 pandas-1.5.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pandas==1.5.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/base.py:329: UserWarning: Trying to unpickle estimator StandardScaler from version 1.0.2 when using version 1.1.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/base.py:329: UserWarning: Trying to unpickle estimator Pipeline from version 1.0.2 when using version 1.1.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/base.py:329: UserWarning: Trying to unpickle estimator ColumnTransformer from version 1.0.2 when using version 1.1.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "pipe = pickle.load(open(\"../raw_data/encoding_pipeline.pkl\", \"rb\"))\n",
    "\n",
    "def encoding(df, columns_to_map):\n",
    "    for i in range(7):\n",
    "        df['position_' + str(i)] = df['nucleotides'].apply(lambda x: x[i])\n",
    "    \n",
    "    df_enc = pd.DataFrame({col: vals for vals, col in zip(pipe.transform(df).T, columns_to_map)})\n",
    "\n",
    "    return df_enc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform pre-processing on dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df):\n",
    "    ## get percentiles\n",
    "    percentile_df = feature_eng(df)\n",
    "    print(f\"After feature engineering, the shape is {percentile_df.shape}\")\n",
    "\n",
    "    ## get relative position\n",
    "    relative_pos_df = relative_position(percentile_df)\n",
    "    print(f\"After finding the relative position, the shape is {relative_pos_df.shape}\")\n",
    "\n",
    "    ## perform encoding\n",
    "    columns_to_map = ['reads_count', 'dwelling_time_-1_25', 'dwelling_time_-1_50', 'dwelling_time_-1_75', \n",
    "                        'dwelling_time_-1_mean', 'dwelling_time_-1_min', 'dwelling_time_-1_max', \n",
    "                        'std_-1_25', 'std_-1_50', 'std_-1_75', 'std_-1_mean', 'std_-1_min', 'std_-1_max', \n",
    "                        'mean_-1_25', 'mean_-1_50', 'mean_-1_75', 'mean_-1_mean', 'mean_-1_min', 'mean_-1_max', \n",
    "                        'dwelling_time_0_25', 'dwelling_time_0_50', 'dwelling_time_0_75', 'dwelling_time_0_mean', \n",
    "                        'dwelling_time_0_min', 'dwelling_time_0_max', 'std_0_25', 'std_0_50', 'std_0_75', 'std_0_mean',\n",
    "                        'std_0_min', 'std_0_max', 'mean_0_25', 'mean_0_50', 'mean_0_75', 'mean_0_mean', 'mean_0_min', \n",
    "                        'mean_0_max', 'dwelling_time_+1_25', 'dwelling_time_+1_50', 'dwelling_time_+1_75', \n",
    "                        'dwelling_time_+1_mean', 'dwelling_time_+1_min', 'dwelling_time_+1_max', 'std_+1_25', 'std_+1_50', \n",
    "                        'std_+1_75', 'std_+1_mean', 'std_+1_min', 'std_+1_max', 'mean_+1_25', 'mean_+1_50', 'mean_+1_75', \n",
    "                        'mean_+1_mean', 'mean_+1_min', 'mean_+1_max', 'relative_position', 'position_0_C', 'position_0_G', \n",
    "                        'position_0_T', 'position_0_A', 'position_1_A', 'position_1_G', 'position_1_T', 'position_2_A', \n",
    "                        'position_2_G', 'position_3_A', 'position_4_C', 'position_5_C', 'position_5_A', 'position_5_T', \n",
    "                        'position_6_T', 'position_6_A', 'position_6_G', 'position_6_C']\n",
    "    for i in range(7):\n",
    "        df['position_' + str(i)] = df['nucleotides'].apply(lambda x: x[i])\n",
    "    df_enc = pd.DataFrame({col: vals for vals, col in zip(pipe.transform(df).T, columns_to_map)})\n",
    "    \n",
    "    enc_df = encoding(relative_pos_df, columns_to_map)\n",
    "    print(f\"After encoding, the shape is {enc_df.shape}\")\n",
    "    \n",
    "    return enc_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['transcript', 'position', 'nucleotides', 'reads_count',\n",
       "       'dwellingtime_-1', 'std_-1', 'mean_-1', 'dwellingtime_0', 'std_0',\n",
       "       'mean_0', 'dwellingtime_+1', 'std_+1', 'mean_+1'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After feature engineering, the shape is (90810, 58)\n",
      "After finding the relative position, the shape is (90810, 59)\n"
     ]
    }
   ],
   "source": [
    "data1_pp = preprocess(dataset1)\n",
    "#data2_pp = preprocess(data2_df)\n",
    "#data3_pp = preprocess(data3_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['reads_count', 'dwelling_time_-1_25', 'dwelling_time_-1_50',\n",
       "       'dwelling_time_-1_75', 'dwelling_time_-1_mean', 'dwelling_time_-1_min',\n",
       "       'dwelling_time_-1_max', 'std_-1_25', 'std_-1_50', 'std_-1_75',\n",
       "       'std_-1_mean', 'std_-1_min', 'std_-1_max', 'mean_-1_25', 'mean_-1_50',\n",
       "       'mean_-1_75', 'mean_-1_mean', 'mean_-1_min', 'mean_-1_max',\n",
       "       'dwelling_time_0_25', 'dwelling_time_0_50', 'dwelling_time_0_75',\n",
       "       'dwelling_time_0_mean', 'dwelling_time_0_min', 'dwelling_time_0_max',\n",
       "       'std_0_25', 'std_0_50', 'std_0_75', 'std_0_mean', 'std_0_min',\n",
       "       'std_0_max', 'mean_0_25', 'mean_0_50', 'mean_0_75', 'mean_0_mean',\n",
       "       'mean_0_min', 'mean_0_max', 'dwelling_time_+1_25',\n",
       "       'dwelling_time_+1_50', 'dwelling_time_+1_75', 'dwelling_time_+1_mean',\n",
       "       'dwelling_time_+1_min', 'dwelling_time_+1_max', 'std_+1_25',\n",
       "       'std_+1_50', 'std_+1_75', 'std_+1_mean', 'std_+1_min', 'std_+1_max',\n",
       "       'mean_+1_25', 'mean_+1_50', 'mean_+1_75', 'mean_+1_mean', 'mean_+1_min',\n",
       "       'mean_+1_max', 'relative_position'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1_pp.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model on full training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load in Train Set\n",
    "X_train_path = \"../data/preprocessed_data/training/X_train_enc.parquet\"\n",
    "X_train = pd.read_parquet(X_train_path)\n",
    "y_train_path = \"../data/preprocessed_data/training/y_train.parquet\"\n",
    "y_train = pd.read_parquet(y_train_path)\n",
    "\n",
    "### convert y_train into int\n",
    "y_train = y_train.values.ravel()\n",
    "y_train = y_train.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfe_features = ['std_-1_25', 'std_-1_50', 'std_-1_75', 'std_-1_mean', 'std_-1_min',\n",
    "       'mean_-1_25', 'mean_-1_50', 'mean_-1_75', 'mean_-1_mean', 'mean_-1_min',\n",
    "       'dwelling_time_0_50', 'dwelling_time_0_mean', 'std_0_25', 'std_0_50',\n",
    "       'std_0_75', 'std_0_mean', 'std_0_min', 'std_0_max', 'mean_0_25',\n",
    "       'mean_0_50', 'mean_0_75', 'mean_0_mean', 'mean_0_min', 'mean_0_max',\n",
    "       'dwelling_time_+1_mean', 'std_+1_25', 'std_+1_50', 'mean_+1_25',\n",
    "       'mean_+1_50', 'mean_+1_75', 'mean_+1_mean', 'mean_+1_min',\n",
    "       'mean_+1_max', 'relative_position', 'position_1_G', 'position_5_T']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier(random_state = 42, n_estimators = 220, max_features = \"sqrt\", max_depth = 30,\n",
    "                        min_samples_split = 2, min_samples_leaf = 1, bootstrap = False)\n",
    "\n",
    "rfc.fit(X_train[rfe_features], y_train)\n",
    "\n",
    "y_test_pred_proba = rfc.predict_proba(dataset1[rfe_features])[:,1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
